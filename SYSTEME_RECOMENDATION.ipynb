{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a41319",
   "metadata": {},
   "source": [
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbab47",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h2>Write by Hyonta B</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031e9c1",
   "metadata": {},
   "source": [
    "<a href=\"https://grouplens.org/datasets/movielens/\" target=\"_parent\">\n",
    "  <img src=\"https://www.svgrepo.com/show/179208/open-sign.svg\" alt=\"Dataset MovieLens\" style=\"width: 50px; height: 50px;\">\n",
    "  open and dwonlode dataset\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb3969",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Chargement des fichiers CSV\n",
    "movies = pd.read_csv('https://raw.githubusercontent.com/hyontnick/model_recomendation/main/data/movies.csv').head(60000)\n",
    "ratings = pd.read_csv('https://raw.githubusercontent.com/hyontnick/model_recomendation/main/data/ratings.csv').head(60000)\n",
    "\n",
    "\n",
    "\n",
    "# Fusionner les données sur 'movieId'\n",
    "data = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Créer la colonne genres_encoded\n",
    "le = LabelEncoder()\n",
    "data['genres_encoded'] = le.fit_transform(data['genres'])\n",
    "\n",
    "# Créer dataset_copy pour les étapes suivantes\n",
    "dataset_copy = data[['userId', 'genres_encoded', 'rating']]\n",
    "\n",
    "# Normaliser les colonnes 'userId' et 'genres_encoded'\n",
    "scaler = MinMaxScaler()\n",
    "dataset_copy[['userId', 'genres_encoded']] = scaler.fit_transform(dataset_copy[['userId', 'genres_encoded']])\n",
    "\n",
    "# Charger le dataset pour Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "surprise_data = Dataset.load_from_df(dataset_copy[['userId', 'genres_encoded', 'rating']], reader)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test pour Surprise\n",
    "trainset, testset = train_test_split(surprise_data, test_size=0.25)\n",
    "\n",
    "# Définir les paramètres à tester pour la recherche en grille\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30, 40],\n",
    "    'lr_all': [0.002, 0.005, 0.007],\n",
    "    'reg_all': [0.02, 0.04, 0.06]\n",
    "}\n",
    "\n",
    "# Faire une recherche en grille pour trouver les meilleurs hyperparamètres\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(surprise_data)\n",
    "\n",
    "# Meilleurs paramètres trouvés\n",
    "best_params = gs.best_params['rmse']\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Entraîner le modèle SVD avec les meilleurs paramètres\n",
    "best_svd = SVD(n_factors=best_params['n_factors'], n_epochs=best_params['n_epochs'],\n",
    "               lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "# Faire des prédictions avec le modèle optimisé\n",
    "svd_predictions_optimized = best_svd.test(testset)\n",
    "\n",
    "# Fonction pour faire une prédiction basée sur le contenu\n",
    "genre_means = data.groupby('genres_encoded')['rating'].mean().to_dict()\n",
    "\n",
    "def content_based_prediction(user_id, genres_encoded):\n",
    "    return genre_means.get(genres_encoded, 3.0)  # Retourner une note moyenne ou 3.0 par défaut\n",
    "\n",
    "# Combiner les prédictions des deux modèles (SV optimisé et modèle basé sur le contenu)\n",
    "hybrid_predictions_optimized = []\n",
    "for pred in svd_predictions_optimized:\n",
    "    content_pred = content_based_prediction(pred.uid, pred.iid)\n",
    "    hybrid_pred = (pred.est + content_pred) / 2  # Combiner en moyenne simple\n",
    "    hybrid_predictions_optimized.append((pred.uid, pred.iid, hybrid_pred, pred.r_ui))\n",
    "\n",
    "# Calculer le RMSE pour les prédictions hybrides optimisées\n",
    "true_ratings_optimized = [pred[3] for pred in hybrid_predictions_optimized]\n",
    "pred_ratings_optimized = [pred[2] for pred in hybrid_predictions_optimized]\n",
    "rmse_optimized = mean_squared_error(true_ratings_optimized, pred_ratings_optimized, squared=False)\n",
    "print(f\"Hybrid RMSE (Optimized): {rmse_optimized}\")\n",
    "\n",
    "# Calculer MAE pour les prédictions hybrides\n",
    "mae_optimized = mean_absolute_error(true_ratings_optimized, pred_ratings_optimized)\n",
    "print(f\"Hybrid MAE (Optimized): {mae_optimized}\")\n",
    "\n",
    "# Fonction pour calculer Precision@k et Recall@k pour les prédictions hybrides\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        user_est_true.setdefault(pred[0], []).append((pred[2], pred[3]))\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "        precisions.append(n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1)\n",
    "        recalls.append(n_rel_and_rec_k / n_rel if n_rel != 0 else 1)\n",
    "\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    return precision, recall\n",
    "\n",
    "# Calculer Precision@k et Recall@k pour les prédictions hybrides optimisées\n",
    "precision_optimized, recall_optimized = precision_recall_at_k(hybrid_predictions_optimized, k=10)\n",
    "print(f\"Hybrid Precision@10 (Optimized): {precision_optimized}\")\n",
    "print(f\"Hybrid Recall@10 (Optimized): {recall_optimized}\")\n",
    "\n",
    "# Fonction pour calculer NDCG@k pour les prédictions hybrides\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    def dcg(relevances, k):\n",
    "        return sum((rel / np.log2(idx + 2) for idx, rel in enumerate(relevances[:k])))\n",
    "\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        user_est_true.setdefault(pred[0], []).append((pred[2], pred[3]))\n",
    "\n",
    "    ndcg_values = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        true_relevances = [true_r for (_, true_r) in user_ratings]\n",
    "        est_relevances = [est for (est, _) in user_ratings]\n",
    "\n",
    "        idcg = dcg(sorted(true_relevances, reverse=True), k)\n",
    "        dcg_value = dcg(est_relevances, k)\n",
    "\n",
    "        ndcg = dcg_value / idcg if idcg != 0 else 0\n",
    "        ndcg_values.append(ndcg)\n",
    "\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "# Calculer NDCG@k pour les prédictions hybrides optimisées\n",
    "ndcg_optimized = ndcg_at_k(hybrid_predictions_optimized, k=10)\n",
    "print(f\"Hybrid NDCG@10 (Optimized): {ndcg_optimized}\")\n",
    "\n",
    "# Fonction pour calculer MAP@k pour les prédictions hybrides\n",
    "def map_at_k(predictions, k=10):\n",
    "    user_est_true = {}\n",
    "    for pred in predictions:\n",
    "        user_est_true.setdefault(pred[0], []).append((pred[2], pred[3]))\n",
    "\n",
    "    map_values = []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        true_positives = 0\n",
    "        precisions = []\n",
    "        for idx, (est, true_r) in enumerate(user_ratings[:k], 1):\n",
    "            if true_r >= 3.5:\n",
    "                true_positives += 1\n",
    "                precisions.append(true_positives / idx)\n",
    "        if precisions:\n",
    "            map_values.append(np.mean(precisions))\n",
    "        else:\n",
    "            map_values.append(0)\n",
    "\n",
    "    return np.mean(map_values)\n",
    "\n",
    "# Calculer MAP@k pour les prédictions hybrides optimisées\n",
    "mapk_optimized = map_at_k(hybrid_predictions_optimized, k=10)\n",
    "print(f\"Hybrid MAP@10 (Optimized): {mapk_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb1605",
   "metadata": {},
   "source": [
    "### Parametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "import pickle\n",
    "\n",
    "# Chargement des fichiers CSV\n",
    "movies = pd.read_csv('ml-25m/movies.csv').head(60000)\n",
    "ratings = pd.read_csv('ml-25m/ratings.csv').head(60000)\n",
    "\n",
    "# Fusionner les données sur 'movieId'\n",
    "data = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# Créer la colonne genres_encoded\n",
    "le = LabelEncoder()\n",
    "data['genres_encoded'] = le.fit_transform(data['genres'])\n",
    "\n",
    "# Créer dataset_copy pour les étapes suivantes\n",
    "dataset_copy = data[['userId', 'genres_encoded', 'rating']]\n",
    "\n",
    "# Normaliser les colonnes 'userId' et 'genres_encoded'\n",
    "scaler = MinMaxScaler()\n",
    "dataset_copy[['userId', 'genres_encoded']] = scaler.fit_transform(dataset_copy[['userId', 'genres_encoded']])\n",
    "\n",
    "# Charger le dataset pour Surprise\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "surprise_data = Dataset.load_from_df(dataset_copy[['userId', 'genres_encoded', 'rating']], reader)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test pour Surprise\n",
    "trainset, testset = train_test_split(surprise_data, test_size=0.25)\n",
    "\n",
    "# Définir les paramètres à tester pour la recherche en grille\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30, 40],\n",
    "    'lr_all': [0.002, 0.005, 0.007],\n",
    "    'reg_all': [0.02, 0.04, 0.06]\n",
    "}\n",
    "\n",
    "# Faire une recherche en grille pour trouver les meilleurs hyperparamètres\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(surprise_data)\n",
    "\n",
    "# Meilleurs paramètres trouvés\n",
    "best_params = gs.best_params['rmse']\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Entraîner le modèle SVD avec les meilleurs paramètres\n",
    "best_svd = SVD(n_factors=best_params['n_factors'], n_epochs=best_params['n_epochs'],\n",
    "               lr_all=best_params['lr_all'], reg_all=best_params['reg_all'])\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "# Sauvegarder le modèle SVD entraîné dans un fichier pickle\n",
    "with open('best_svd_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_svd, model_file)\n",
    "\n",
    "# Calculer les moyennes des genres pour le modèle basé sur le contenu\n",
    "genre_means = data.groupby('genres_encoded')['rating'].mean().to_dict()\n",
    "\n",
    "# Sauvegarder les moyennes des genres dans un fichier pickle\n",
    "with open('genre_means.pkl', 'wb') as genre_file:\n",
    "    pickle.dump(genre_means, genre_file)\n",
    "\n",
    "print(\"Les modèles ont été sauvegardés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea34ea0",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba33ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from surprise import SVD\n",
    "\n",
    "# Charger les fichiers sauvegardés\n",
    "with open('best_svd_model.pkl', 'rb') as model_file:\n",
    "    best_svd = pickle.load(model_file)\n",
    "\n",
    "with open('genre_means.pkl', 'rb') as genre_file:\n",
    "    genre_means = pickle.load(genre_file)\n",
    "\n",
    "# Fonction pour faire une prédiction basée sur le contenu\n",
    "def content_based_prediction(user_id, genres_encoded):\n",
    "    return genre_means.get(genres_encoded, 3.0)  # Retourner une note moyenne ou 3.0 par défaut\n",
    "\n",
    "# Fonction pour faire des prédictions hybrides\n",
    "def hybrid_prediction(user_id, genres_encoded):\n",
    "    svd_pred = best_svd.predict(user_id, genres_encoded).est\n",
    "    content_pred = content_based_prediction(user_id, genres_encoded)\n",
    "    hybrid_pred = (svd_pred + content_pred) / 2\n",
    "    return hybrid_pred\n",
    "\n",
    "# Exemple d'utilisation du script de prédiction\n",
    "if __name__ == \"__main__\":\n",
    "    # Charger les données (ici, on utilise un échantillon pour la démonstration)\n",
    "    ratings = pd.read_csv('ml-25m/ratings.csv').head(60000)\n",
    "    movies = pd.read_csv('ml-25m/movies.csv').head(60000)\n",
    "    data = pd.merge(ratings, movies, on='movieId')\n",
    "    le = LabelEncoder()\n",
    "    data['genres_encoded'] = le.fit_transform(data['genres'])\n",
    "    \n",
    "    # Normaliser les colonnes 'userId' et 'genres_encoded'\n",
    "    scaler = MinMaxScaler()\n",
    "    data[['userId', 'genres_encoded']] = scaler.fit_transform(data[['userId', 'genres_encoded']])\n",
    "    \n",
    "    user_id = int(input(\"Entrez l'ID de l'utilisateur: \"))\n",
    "    movie_id = int(input(\"Entrez l'ID du film: \"))\n",
    "    \n",
    "    genres_encoded = data[data['movieId'] == movie_id]['genres_encoded'].values[0]\n",
    "    \n",
    "    prediction = hybrid_prediction(user_id, genres_encoded)\n",
    "    print(f\"La prédiction hybride pour l'utilisateur {user_id} et le film {movie_id} est {prediction:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
